IN THIS BEGINNER ARTICLE, WE’LL LOOK AT THE 10 PRACTICAL USE CASES OF STREAMS IN NODE.JS. WITHOUT FURTHER DELAY, LET’S GET STARTED.

USE CASE 1 — FILE I/O
STREAMS ENABLE EFFICIENT FILE I/O OPERATIONS, ESPECIALLY WHEN DEALING WITH LARGE FILES. LOADING ENTIRE FILE CONTENTS INTO MEMORY CAN BE IMPRACTICAL OR IMPOSSIBLE. INSTEAD, STREAMS ALLOW PROCESSING FILES CHUNK BY CHUNK, REDUCING MEMORY OVERHEAD.

FOR EXAMPLE, USE THE FS MODULE'S CREATEREADSTREAM() METHOD TO READ A LARGE FILE AS A STREAM:

CONST FS = REQUIRE('FS');

CONST READSTREAM = FS.CREATEREADSTREAM('LARGE_FILE.TXT');

READSTREAM.ON('DATA', (CHUNK) => {
  CONSOLE.LOG(`RECEIVED CHUNK: ${CHUNK}`);
  // PROCESS THE CHUNK
});

READSTREAM.ON('END', () => {
  CONSOLE.LOG('FILE READ COMPLETE');
});
SIMILARLY, USE CREATEWRITESTREAM() TO WRITE TO A FILE AS A STREAM:

CONST FS = REQUIRE('FS');

CONST READSTREAM = FS.CREATEREADSTREAM('LARGE_FILE.TXT');

READSTREAM.ON('DATA', (CHUNK) => {
  CONSOLE.LOG(`RECEIVED CHUNK: ${CHUNK}`);
  // PROCESS THE CHUNK
});

READSTREAM.ON('END', () => {
  CONSOLE.LOG('FILE READ COMPLETE');
});
BY USING STREAMS FOR FILE I/O, WE CAN EFFICIENTLY PROCESS LARGE FILES WITHOUT LOADING ENTIRE CONTENTS INTO MEMORY.

USE CASE 2 — HTTP RESPONSES
STREAMS ENABLE EFFICIENT HANDLING OF LARGE AMOUNTS OF DATA IN HTTP RESPONSES BY ALLOWING DIRECT STREAMING WITHOUT BUFFERING EVERYTHING IN MEMORY. THIS APPROACH PREVENTS MEMORY OVERLOAD AND ENABLES REAL-TIME DATA PROCESSING.

FOR EXAMPLE, CHECK OUT THE FOLLOWING HTTP SERVER THAT STREAMS DATA DIRECTLY TO THE RESPONSE:

CONST HTTP = REQUIRE('HTTP');

HTTP.CREATESERVER((REQ, RES) => {
  CONST STREAM = FS.CREATEREADSTREAM('LARGE_FILE.TXT');
  STREAM.PIPE(RES);
}).LISTEN(3000);
IN THIS EXAMPLE, WE HAVE CREATED AN HTTP SERVER THAT STREAMS THE CONTENTS OF A LARGE FILE DIRECTLY TO THE RESPONSE USING THE PIPE() METHOD. THIS APPROACH ENABLES EFFICIENT HANDLING OF LARGE FILES WITHOUT LOADING THEM ENTIRELY INTO MEMORY.

BY USING STREAMS FOR HTTP RESPONSES, WE CAN EFFICIENTLY HANDLE LARGE AMOUNTS OF DATA AND PREVENT MEMORY OVERLOAD, ENABLING REAL-TIME DATA PROCESSING AND IMPROVED PERFORMANCE.

USE CASE 3 — DATA PROCESSING
STREAMS ENABLE BUILDING DATA PROCESSING PIPELINES WHERE DATA FLOWS THROUGH MULTIPLE STAGES, EACH PROCESSING A PART OF THE DATA AS IT BECOMES AVAILABLE. THIS APPROACH ALLOWS FOR EFFICIENT, REAL-TIME DATA PROCESSING AND TRANSFORMATION.

TAKE A LOOK AT THE FOLLOWING EXAMPLES THAT USES THE STREAM MODULE TO CREATE A PIPELINE THAT PROCESSES DATA IN MULTIPLE STAGES:

CONST STREAM = REQUIRE('STREAM');

CONST STAGE1 = NEW STREAM.TRANSFORM({
  TRANSFORM(CHUNK, ENCODING, CALLBACK) {
    // PROCESS CHUNK
    CALLBACK(NULL, CHUNK);
  }
});

CONST STAGE2 = NEW STREAM.TRANSFORM({
  TRANSFORM(CHUNK, ENCODING, CALLBACK) {
    // PROCESS CHUNK
    CALLBACK(NULL, CHUNK);
  }
});

CONST STAGE3 = NEW STREAM.TRANSFORM({
  TRANSFORM(CHUNK, ENCODING, CALLBACK) {
    // PROCESS CHUNK
    CALLBACK(NULL, CHUNK);
  }
});

READSTREAM.PIPE(STAGE1).PIPE(STAGE2).PIPE(STAGE3).PIPE(WRITESTREAM);
IN THIS EXAMPLE, A PIPELINE WAS CREATED WITH THREE STAGES, EACH PROCESSING A PART OF THE DATA AS IT BECOMES AVAILABLE. THE PIPE() METHOD CONNECTS EACH STAGE, ENABLING DATA TO FLOW THROUGH THE PIPELINE.

BY USING STREAMS FOR DATA PROCESSING PIPELINES, WE CAN BUILD EFFICIENT, REAL-TIME DATA PROCESSING WORKFLOWS THAT SCALE AND HANDLE LARGE AMOUNTS OF DATA WITH EASE.

USE CASE 4 — REAL-TIME DATA
STREAMS ENABLE HANDLING REAL-TIME DATA SOURCES LIKE SENSOR DATA, LOGS, OR NETWORK PACKETS BY PROCESSING THEM AS THEY ARRIVE, WITHOUT WAITING FOR THE ENTIRE DATASET. THIS APPROACH ALLOWS FOR IMMEDIATE INSIGHTS, TIMELY REACTIONS, AND EFFICIENT PROCESSING.

FOR EXAMPLE, USE THE STREAM MODULE TO PROCESS REAL-TIME SENSOR DATA:

CONST SENSORSTREAM = NEW STREAM.PASSTHROUGH();

SENSORSTREAM.ON('DATA', (CHUNK) => {
  // PROCESS CHUNK IMMEDIATELY
  CONSOLE.LOG(`RECEIVED SENSOR DATA: ${CHUNK}`);
});

// SIMULATE SENSOR DATA ARRIVAL
SETINTERVAL(() => {
  SENSORSTREAM.WRITE('SENSOR DATA PACKET');
}, 1000);
IN THIS EXAMPLE, WE HAVE CREATED A STREAM TO PROCESS SENSOR DATA AS IT ARRIVES. THE CODE LISTENS FOR THE DATA EVENT AND PROCESS EACH CHUNK IMMEDIATELY. THE SETINTERVAL() FUNCTION SIMULATES SENSOR DATA ARRIVAL EVERY SECOND.

USE CASE 5 — COMPRESSION/DECOMPRESSION
STREAMS ENABLE ON-THE-FLY COMPRESSION AND DECOMPRESSION OF DATA, REDUCING MEMORY FOOTPRINT AND IMPROVING PERFORMANCE WHEN DEALING WITH LARGE DATASETS. THIS APPROACH ALLOWS FOR EFFICIENT DATA PROCESSING AND TRANSFER.

THE FOLLOWING EXAMPLE USES THE ZLIB MODULE TO COMPRESS AND DECOMPRESS DATA USING STREAMS:

CONST ZLIB = REQUIRE('ZLIB');
CONST COMPRESSSTREAM = ZLIB.CREATEGZIP();
CONST DECOMPRESSSTREAM = ZLIB.CREATEGUNZIP();

READSTREAM.PIPE(COMPRESSSTREAM).PIPE(WRITESTREAM);
READSTREAM.PIPE(DECOMPRESSSTREAM).PIPE(WRITESTREAM);
IN THIS EXAMPLE, COMPRESSION AND DECOMPRESSION STREAMS WERE CREATED USING ZLIB. THE CODE PIPES THE READ STREAM TO THE COMPRESSION STREAM, WHICH COMPRESSES THE DATA ON-THE-FLY, AND PIPES THE COMPRESSED DATA TO THE WRITE STREAM. SIMILARLY, WE PIPE THE READ STREAM TO THE DECOMPRESSION STREAM, WHICH DECOMPRESSES THE DATA ON-THE-FLY, AND PIPE THE DECOMPRESSED DATA TO THE WRITE STREAM.

BY USING STREAMS FOR COMPRESSION AND DECOMPRESSION, WE CAN EFFICIENTLY PROCESS AND TRANSFER LARGE DATASETS, REDUCING MEMORY USAGE AND IMPROVING PERFORMANCE.

USE CASE 6 — DATABASE OPERATIONS
STREAMS ENABLE EFFICIENT DATA TRANSFER BETWEEN DATABASES AND NODE.JS APPLICATIONS, ESPECIALLY WHEN DEALING WITH LARGE RESULT SETS OR BULK DATA INSERTS/UPDATES. THIS APPROACH REDUCES MEMORY OVERHEAD AND IMPROVES PERFORMANCE.

FOR EXAMPLE, THE FOLLOWING SNIPPET USES THE MYSQL MODULE TO STREAM DATA FROM A DATABASE QUERY:

CONST MYSQL = REQUIRE('MYSQL');

CONST DB = MYSQL.CREATECONNECTION({
  HOST: '(LINK UNAVAILABLE)',
  USER: 'USERNAME',
  PASSWORD: 'PASSWORD',
  DATABASE: 'DATABASE'
});

DB.QUERY('SELECT * FROM LARGE_TABLE')
  .STREAM()
  .PIPE(WRITESTREAM);
IN THE ABOVE EXAMPLE, WE HAVE CREATED A MYSQL CONNECTION AND QUERY A LARGE TABLE. THIS FOLLOWS BY A CALL TO STREAM() ON THE QUERY RESULT, WHICH RETURNS A READABLE STREAM. THE RESULT IS PIPED TO A WRITE STREAM, WHICH WRITES THE DATA TO A FILE OR ANOTHER DESTINATION.

BY USING STREAMS FOR DATABASE OPERATIONS, WE CAN EFFICIENTLY TRANSFER LARGE DATASETS, REDUCING MEMORY USAGE AND IMPROVING PERFORMANCE. THIS APPROACH IS PARTICULARLY USEFUL FOR BULK DATA INSERTS, UPDATES, AND DATA MIGRATIONS.

USE CASE 7 — CONCURRENCY CONTROL
STREAMS ENABLE CONCURRENCY CONTROL BY MANAGING THE FLOW OF DATA, ENSURING OPTIMAL RESOURCE UTILIZATION AND PREVENTING MEMORY EXHAUSTION. THIS APPROACH ALLOWS FOR EFFICIENT HANDLING OF MULTIPLE CONCURRENT OPERATIONS, SUCH AS HANDLING MULTIPLE REQUESTS OR PROCESSING LARGE DATASETS IN PARALLEL.

CHECK THE FOLLOWING EXAMPLE THAT USES THE STREAM MODULE TO CONTROL THE CONCURRENCY OF DATA PROCESSING:

CONST STREAM = REQUIRE('STREAM');
CONST CONCURRENTLIMIT = 5;

CONST CONCURRENCYCONTROLSTREAM = NEW STREAM.PASSTHROUGH({
  CONCURRENT: CONCURRENTLIMIT,
});

READSTREAM.PIPE(CONCURRENCYCONTROLSTREAM).PIPE(WRITESTREAM);
IN THIS EXAMPLE, A PASS-THROUGH STREAM WAS CREATED WITH A CONCURRENCY LIMIT OF 5. THIS MEANS THAT ONLY 5 CHUNKS OF DATA WILL BE PROCESSED CONCURRENTLY. IF MORE DATA IS AVAILABLE, IT WILL BE BUFFERED UNTIL ONE OF THE CONCURRENT PROCESSING SLOTS BECOMES AVAILABLE.

